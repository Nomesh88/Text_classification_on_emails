{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the required model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#---------------------------------------Text Processing------------------------------------------------------------#\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "#------------------------------------Metrics and Validation---------------------------------------------------------#\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "#-------------------------------------Models to be trained----------------------------------------------------------#\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading all the files and converting every .txt file to one column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crime', 'Entertainment', 'Politics', 'Science']\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "base = 'C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/text_classification_email/Training_data/'\n",
    "with os.scandir(base) as entries:\n",
    "    for entry in entries:\n",
    "        if(entry.is_file() == False):\n",
    "            names.append(entry.name)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "unique = []\n",
    "for name in names:\n",
    "    path = base + name+'/'\n",
    "    x = []\n",
    "    with os.scandir(path) as entries:\n",
    "        for entry in entries:\n",
    "            if(entry.is_file()):\n",
    "                x.append(entry.name)\n",
    "    files[name] = x\n",
    "    files[name].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(names)):\n",
    "    x = files[names[i]]\n",
    "    for j in x:\n",
    "        for k in range(i+1, len(names)):\n",
    "            key = names[k]\n",
    "            if j in files[key]:\n",
    "                files[key].remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6734, 2)\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "i = 0\n",
    "\n",
    "for genre in files.keys() :\n",
    "    texts = files[genre]\n",
    "    for text in texts:\n",
    "        if text in files[genre]:\n",
    "            path = base + genre + '/' + text\n",
    "            with open(path, \"r\", encoding = \"latin1\") as file:\n",
    "                data[i] = file.readlines()\n",
    "                i = i+1\n",
    "            data[i-1] = [\" \".join(data[i-1]), genre] \n",
    "\n",
    "data = pd.DataFrame(data).T\n",
    "print(data.shape)\n",
    "data.columns = ['Text', 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = list(data.Text.unique())\n",
    "len(unique)\n",
    "dic = dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique values in the documents\n",
    "uni = {}\n",
    "i = 0\n",
    "for k in range(len(list(dic['Text']))):\n",
    "    if dic['Text'][k] in unique:\n",
    "        uni[i] = [dic['Text'][k], dic['Class'][k]]\n",
    "        unique.remove(dic['Text'][k])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the dataset and classifying the data to classnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6581, 2)\n"
     ]
    }
   ],
   "source": [
    "#classification into text and type of class\n",
    "data = pd.DataFrame(uni).T\n",
    "print(data.shape)\n",
    "data.columns = ['Text', 'Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\nomesh.palakalur\n",
      "[nltk_data]     i.EMEA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#cleaning text\n",
    "import nltk.corpus\n",
    "import regex\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "for punct in punctuation:\n",
    "    stop.append(punct)\n",
    "\n",
    "def filter_text(text, stop_words):\n",
    "    word_tokens = WordPunctTokenizer().tokenize(text.lower())\n",
    "    filtered_text = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha() and len(w) > 3]\n",
    "    filtered_text = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in filtered_text if not w in stop_words] \n",
    "    return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\nomesh.palakaluri.\n",
      "[nltk_data]     EMEA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\nomesh.palakaluri.\n",
      "[nltk_data]     EMEA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#applying the filter method on origibal text\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "data[\"filtered_text\"] = data.Text.apply(lambda x : filter_text(x, stop)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S aving the vectorizer in a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#vectorizing the data \n",
    "tfidf=TfidfVectorizer()\n",
    "filename='tfidf.pick'\n",
    "pickle.dump(tfidf, open(filename, 'wb'))\n",
    "\n",
    "#train_vec = tfidf.fit_transform(data['filtered_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the vectorizer from the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2176)\t0.010470275703028841\n",
      "  (0, 22982)\t0.01202214230128693\n",
      "  (0, 32697)\t0.011832176202765212\n",
      "  (0, 24510)\t0.01719194997996771\n",
      "  (0, 30202)\t0.015817794021327238\n",
      "  (0, 26902)\t0.01194453878780885\n",
      "  (0, 2758)\t0.01981421845998446\n",
      "  (0, 7414)\t0.020147292375635215\n",
      "  (0, 6275)\t0.010661140738426644\n",
      "  (0, 42262)\t0.011301265790630042\n",
      "  (0, 9555)\t0.015239129658539886\n",
      "  (0, 34954)\t0.011024874268906236\n",
      "  (0, 23487)\t0.009761664697026284\n",
      "  (0, 11121)\t0.015465319009974733\n",
      "  (0, 11993)\t0.015561403101600771\n",
      "  (0, 17590)\t0.00994928362133883\n",
      "  (0, 27191)\t0.011164287255451507\n",
      "  (0, 27395)\t0.008993449465875736\n",
      "  (0, 2004)\t0.013847674163902018\n",
      "  (0, 39533)\t0.009943218075191454\n",
      "  (0, 35537)\t0.012688838577964096\n",
      "  (0, 36479)\t0.023386061783693467\n",
      "  (0, 6807)\t0.010411952148509002\n",
      "  (0, 12316)\t0.013186029660684427\n",
      "  (0, 19263)\t0.018566105938608184\n",
      "  :\t:\n",
      "  (6580, 1237)\t0.059367537143380084\n",
      "  (6580, 16832)\t0.039859353677237634\n",
      "  (6580, 9565)\t0.05723606283997494\n",
      "  (6580, 3648)\t0.04486931631270501\n",
      "  (6580, 35568)\t0.07480703469535821\n",
      "  (6580, 40171)\t0.09819388936508867\n",
      "  (6580, 14393)\t0.054700789210768944\n",
      "  (6580, 34954)\t0.05322038145643746\n",
      "  (6580, 23487)\t0.09424497842860546\n",
      "  (6580, 31402)\t0.04051639269984734\n",
      "  (6580, 13017)\t0.04609780212433672\n",
      "  (6580, 33615)\t0.03189902261413372\n",
      "  (6580, 27464)\t0.05637194570698493\n",
      "  (6580, 38530)\t0.058779427762864794\n",
      "  (6580, 21609)\t0.046878744987773315\n",
      "  (6580, 7267)\t0.03451617798080955\n",
      "  (6580, 17438)\t0.04650845355949295\n",
      "  (6580, 5548)\t0.10270090523727264\n",
      "  (6580, 41873)\t0.06451904634581765\n",
      "  (6580, 18585)\t0.04161346856201799\n",
      "  (6580, 30389)\t0.15220018637658078\n",
      "  (6580, 26601)\t0.04319000967913949\n",
      "  (6580, 21140)\t0.026310478580971703\n",
      "  (6580, 42496)\t0.05344367178587581\n",
      "  (6580, 22333)\t0.0505431675026754\n"
     ]
    }
   ],
   "source": [
    "loaded_vec = pickle.load(open(filename, 'rb'))\n",
    "train_vec = loaded_vec.fit_transform(data['filtered_text'])\n",
    "print(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifying the classes and assigning some values\n",
    "data['classification'] = data['Class'].replace(['Crime','Politics','Science'],[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting and train of the data\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_vec,data['classification'], stratify=data['classification'], test_size=0.2)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "C = np.arange(0, 1, 0.001)\n",
    "max_iter = range(100, 500)\n",
    "warm_start = [True, False]\n",
    "solver = ['lbfgs', 'newton-cg', 'liblinear']\n",
    "penalty = ['l2', 'l1']\n",
    "\n",
    "params = {\n",
    "    'C' : C,\n",
    "    'max_iter' : max_iter,\n",
    "    'warm_start' : warm_start,\n",
    "    'solver' : solver,\n",
    "    'penalty' : penalty\n",
    "}\n",
    "#log='C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/Text model/finalized_model.sav'\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator =LogisticRegression(random_state=1),\n",
    "    param_distributions = params,\n",
    "    n_iter = 100,\n",
    "    cv = 3,\n",
    "    n_jobs = -1,\n",
    "    random_state = 1,\n",
    "    verbose = 1\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9791033434650456"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "model_lr = random_search.best_estimator_\n",
    "model_lr.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S. No.</th>\n",
       "      <th>Message</th>\n",
       "      <th>Label</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UpgrdCentre Orange customer, you may now claim...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>upgrdcentre orange customer claim free camera ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Loan for any purpose £500 - £75,000. Homeowner...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>loan purpose homeowners tenant welcome previou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Congrats! Nokia 3650 video camera phone is you...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>congrats nokia video camera phone call call co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>URGENT! Your Mobile number has been awarded wi...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>urgent mobile number award prize guarantee cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Someone has contacted our dating service and e...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>someone contact date service enter phone fancy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S. No.                                            Message Label  \\\n",
       "0       1  UpgrdCentre Orange customer, you may now claim...  Spam   \n",
       "1       2  Loan for any purpose £500 - £75,000. Homeowner...  Spam   \n",
       "2       3  Congrats! Nokia 3650 video camera phone is you...  Spam   \n",
       "3       4  URGENT! Your Mobile number has been awarded wi...  Spam   \n",
       "4       5  Someone has contacted our dating service and e...  Spam   \n",
       "\n",
       "                                       filtered_text  \n",
       "0  upgrdcentre orange customer claim free camera ...  \n",
       "1  loan purpose homeowners tenant welcome previou...  \n",
       "2  congrats nokia video camera phone call call co...  \n",
       "3  urgent mobile number award prize guarantee cal...  \n",
       "4  someone contact date service enter phone fancy...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing data\n",
    "test_data=pd.read_csv(f'C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/text_classification_email/Testing_data/SMS_test.csv',encoding='unicode_escape')\n",
    "test_data[\"filtered_text\"] = test_data['Message'].apply(lambda x : filter_text(x, stop)) \n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing the data\n",
    "test_vec=loaded_vec.transform(test_data['filtered_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 1 2 2 2\n",
      " 2 2 2 2 2 2 1 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2\n",
      " 2 2 2 1 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1\n",
      " 2 2 2 2 2 1 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "#predicting the output\n",
    "pred=model_lr.predict(test_vec)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "# model_mnb.predict(x_val)\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "# sen1=[\"hi i am excited a good news with you\",'wish you good luck today','fine get on with it']\n",
    "# sen1=tfidf.transform(sen1[0].split())\n",
    "# model_mnb.predict(sen1)\n",
    "b = Counter(model_lr.predict(test_vec))\n",
    "if(b.most_common()[0][0]==0):\n",
    "    print('Crime')\n",
    "elif(b.most_common()[0][0]==1):\n",
    "    print('Politics')\n",
    "else:\n",
    "    print('Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"\"\"\n",
    "With visa office closures and international travel restrictions still in place, experts say the economic benefits of President Donald Trump’s immigration suspension last week are uncertain, at best, while thousands of prospective immigrants still stand to suffer the consequences.\n",
    "\n",
    "Although the suspension was filled with broad exemptions and does not include current visa holders or those already in the U.S., the Migration Policy Institute estimates 26,000 would-be green card applicants would be blocked each month.\n",
    "\n",
    "Over the weekend, a coalition of advocacy groups filed an emergency request to halt Trump’s directive. It was the latest move in a 2019 case, stemming from when the White House sought to ban immigrants who couldn’t prove their ability to acquire.\n",
    "Attorneys with the American Immigration Lawyers Association requested a hold on the recent suspension order while the courts continue to decide the fate of the health insurance restrictions.\n",
    "\n",
    "The White House framed the immigration suspension as a vital component of helping the economy and American workers recover, but it could have the opposite effect.\n",
    "\n",
    "Immigrants already in the U.S. and those who already have certain visas can still obtain permanent residency. The suspension applies only to green card applicants from outside the country and primarily targets to those looking to settle down in the U.S. permanently through a family connection, according to MPI estimates.\n",
    "\n",
    "That leaves the possibility of employment-based visa slots opening up as a result of the suspension.\n",
    "\n",
    "\"President Trump claims he signed it in order to protect the economy in the wake of the coronavirus outbreak, but the reality is that our economic recovery will depend on immigrants,\" said Esther Sung, a senior attorney involved in the 2019 case.\n",
    "\n",
    "That view is shared, in part, by Rutgers economics professor Jennifer Hunt. She told ABC News that an influx of workers into critical sectors of the economy could boost per capita GDP.\n",
    "\n",
    "\"It would be kind of a shot in the arm,\" Hunt said.\n",
    "\n",
    "On-going travel restrictions and the closure of foreign visa offices still pose the greatest barrier to legal immigration in the age of coronavirus. Economists and labor experts, including Hunt, say those factors would override any attempt to assess the economic impacts of the restrictions.\n",
    "\n",
    "\"It’s very hard for me to see the direct correlation in terms of immediate and practical positive impact on displaced U.S. workers,\" said Caroline Tang, an Austin-based attorney who advises companies on work authorization for immigrants.\n",
    "\n",
    "Tang said the bar for employers to obtain work authorization for visa holders is already high enough, referring to specific requirements for prioritizing U.S. citizens.\n",
    "\n",
    "Researchers have identified some connection between more immigrants settled in the country and economic growth under normal circumstances, but the results are mixed.\n",
    "\n",
    "For example, a 2017 report by the economics firm Moody Analytics and ProPublica found that for every 1% increase in the U.S. population, the gross domestic product rises by 1.5%. While a 2018 study from Stanford University’s Hoover Institution confirms that \"positive relationship\" generally, but found variations and some negative impacts to U.S.-born employment rates in its state-by-state review.\n",
    "\n",
    "Researchers studying the large population of foreign-born workers in South Africa said it likely had a positive impact on per capita GDP.\n",
    "\n",
    "\"Foreign-born workers also generated additional employment for native-born workers,\" according to the 2018 Organisation for Economic Co-operation and Development report.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing the test data\n",
    "test_vec1=loaded_vec.transform([text_1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "# model_mnb.predict(x_val)\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "# sen1=[\"hi i am excited a good news with you\",'wish you good luck today','fine get on with it']\n",
    "# sen1=tfidf.transform(sen1[0].split())\n",
    "# model_mnb.predict(sen1)\n",
    "b = Counter(model_lr.predict(test_vec1))\n",
    "if(b.most_common()[0][0]==0):\n",
    "    print('Crime')\n",
    "elif(b.most_common()[0][0]==1):\n",
    "    print('Politics')\n",
    "else:\n",
    "    print('Science')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ac7df784fb9f66341270b4235b901a57e4c94a02ff00519a90c3611eb6bf397"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
