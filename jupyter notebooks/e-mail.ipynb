{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#---------------------------------------Text Processing------------------------------------------------------------#\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "#------------------------------------Metrics and Validation---------------------------------------------------------#\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "#-------------------------------------Models to be trained----------------------------------------------------------#\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crime', 'Entertainment', 'Politics', 'Science']\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "base = 'C:/Users/nomesh.palakaluri.EMEA/OneDrive - Drilling Info/Desktop/Text model/Data/'\n",
    "with os.scandir(base) as entries:\n",
    "    for entry in entries:\n",
    "        if(entry.is_file() == False):\n",
    "            names.append(entry.name)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "unique = []\n",
    "for name in names:\n",
    "    path = base + name+'/'\n",
    "    x = []\n",
    "    with os.scandir(path) as entries:\n",
    "        for entry in entries:\n",
    "            if(entry.is_file()):\n",
    "                x.append(entry.name)\n",
    "    files[name] = x\n",
    "    files[name].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(names)):\n",
    "    x = files[names[i]]\n",
    "    for j in x:\n",
    "        for k in range(i+1, len(names)):\n",
    "            key = names[k]\n",
    "            if j in files[key]:\n",
    "                files[key].remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6734, 2)\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "i = 0\n",
    "\n",
    "for genre in files.keys() :\n",
    "    texts = files[genre]\n",
    "    for text in texts:\n",
    "        if text in files[genre]:\n",
    "            path = base + genre + '/' + text\n",
    "            with open(path, \"r\", encoding = \"latin1\") as file:\n",
    "                data[i] = file.readlines()\n",
    "                i = i+1\n",
    "            data[i-1] = [\" \".join(data[i-1]), genre] \n",
    "\n",
    "data = pd.DataFrame(data).T\n",
    "print(data.shape)\n",
    "data.columns = ['Text', 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6581"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = list(data.Text.unique())\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique values in the documents\n",
    "uni = {}\n",
    "i = 0\n",
    "for k in range(len(list(dic['Text']))):\n",
    "    if dic['Text'][k] in unique:\n",
    "        uni[i] = [dic['Text'][k], dic['Class'][k]]\n",
    "        unique.remove(dic['Text'][k])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6581, 2)\n"
     ]
    }
   ],
   "source": [
    "#classification into text and type of class\n",
    "data = pd.DataFrame(uni).T\n",
    "print(data.shape)\n",
    "data.columns = ['Text', 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\nomesh.palakalur\n",
      "[nltk_data]     i.EMEA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#cleaning text\n",
    "import nltk.corpus\n",
    "import regex\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "for punct in punctuation:\n",
    "    stop.append(punct)\n",
    "\n",
    "def filter_text(text, stop_words):\n",
    "    word_tokens = WordPunctTokenizer().tokenize(text.lower())\n",
    "    filtered_text = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha() and len(w) > 3]\n",
    "    filtered_text = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in filtered_text if not w in stop_words] \n",
    "    return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\nomesh.palakaluri.\n",
      "[nltk_data]     EMEA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\nomesh.palakaluri.\n",
      "[nltk_data]     EMEA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n Archive-name: ripem/faq\\n Last-update: Sun,...</td>\n",
       "      <td>Crime</td>\n",
       "      <td>archive name ripem last update post still rath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Approved: news-answers-request@MIT.EDU\\n Conte...</td>\n",
       "      <td>Crime</td>\n",
       "      <td>approve news answer request content type text ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Approved: news-answers-request@MIT.EDU\\n Conte...</td>\n",
       "      <td>Crime</td>\n",
       "      <td>approve news answer request content type text ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Message-ID: &lt;1ppvai$l79@bilbo.suite.com&gt;\\n Rep...</td>\n",
       "      <td>Crime</td>\n",
       "      <td>message bilbo suite reply miller suite nntp po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n    Some sick part of me really liked that p...</td>\n",
       "      <td>Crime</td>\n",
       "      <td>sick part really like phrase actually merely t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class  \\\n",
       "0  \\n Archive-name: ripem/faq\\n Last-update: Sun,...  Crime   \n",
       "1  Approved: news-answers-request@MIT.EDU\\n Conte...  Crime   \n",
       "2  Approved: news-answers-request@MIT.EDU\\n Conte...  Crime   \n",
       "3  Message-ID: <1ppvai$l79@bilbo.suite.com>\\n Rep...  Crime   \n",
       "4  \\n    Some sick part of me really liked that p...  Crime   \n",
       "\n",
       "                                       filtered_text  \n",
       "0  archive name ripem last update post still rath...  \n",
       "1  approve news answer request content type text ...  \n",
       "2  approve news answer request content type text ...  \n",
       "3  message bilbo suite reply miller suite nntp po...  \n",
       "4  sick part really like phrase actually merely t...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "data[\"filtered_text\"] = data.Text.apply(lambda x : filter_text(x, stop)) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             words\n",
      "17      encryption\n",
      "19           write\n",
      "53            know\n",
      "79             use\n",
      "96           write\n",
      "...            ...\n",
      "154707         use\n",
      "154734       would\n",
      "154762       would\n",
      "154765       would\n",
      "154767        know\n",
      "\n",
      "[8874 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#top 10 words to claasify the text into crime class\n",
    "all_text = \" \".join(data[data.Class == \"Crime\"].filtered_text) \n",
    "count = pd.DataFrame(all_text.split(), columns = ['words'])\n",
    "top_10 = count[count['words'].isin(list(count.words.value_counts()[:10].index[:10]))]\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         words\n",
      "10       write\n",
      "13       state\n",
      "27       would\n",
      "38         say\n",
      "45       state\n",
      "...        ...\n",
      "537437   state\n",
      "537444   state\n",
      "537516  people\n",
      "537544   would\n",
      "537556   right\n",
      "\n",
      "[29328 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#top 10 words to claasify the text into politics class\n",
    "all_text = \" \".join(data[data.Class == \"Politics\"].filtered_text)\n",
    "count = pd.DataFrame(all_text.split(), columns = ['words'])\n",
    "top_10 = count[count['words'].isin(list(count.words.value_counts()[:10].index[:10]))]\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          words\n",
      "1          post\n",
      "13      article\n",
      "43        would\n",
      "52         know\n",
      "54          use\n",
      "...         ...\n",
      "327941    write\n",
      "327997     know\n",
      "328006    space\n",
      "328017    space\n",
      "328021    space\n",
      "\n",
      "[15315 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#top 10 words to claasify the text into science class\n",
    "all_text = \" \".join(data[data.Class == \"Science\"].filtered_text)\n",
    "count = pd.DataFrame(all_text.split(), columns = ['words'])\n",
    "top_10 = count[count['words'].isin(list(count.words.value_counts()[:10].index[:10]))]\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6581, 43129)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False)\n",
    "train_vec = tfidf.fit_transform(data['filtered_text'])\n",
    "train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['classification'] = data['Class'].replace(['Crime','Politics','Science'],[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_vec,data['classification'], stratify=data['classification'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "117 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "42 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\nomesh.palakaluri.EMEA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.92876237 0.87423899        nan        nan        nan 0.75645834\n",
      " 0.90577597 0.82940689 0.92173331 0.8949476         nan 0.923253\n",
      "        nan        nan        nan 0.80604062        nan 0.87462027\n",
      " 0.82921684 0.92876237 0.91698432 0.8721494  0.43825991        nan\n",
      " 0.92116362        nan 0.91603466        nan 0.92895241        nan\n",
      " 0.91926418 0.9086253         nan 0.92572267        nan 0.9002666\n",
      "        nan        nan        nan 0.58796673        nan 0.89798686\n",
      " 0.91261477 0.86778006 0.84916344 0.92800253 0.82978675        nan\n",
      " 0.83909523        nan 0.88430883 0.93028238 0.91850433 0.81781888\n",
      " 0.90653592        nan 0.91926418        nan 0.91983398        nan\n",
      " 0.87461886 0.92572267 0.9122349  0.88240895 0.84859364 0.90710594\n",
      "        nan 0.83909566        nan        nan 0.90596591        nan\n",
      "        nan 0.91299474        nan 0.91812425 0.8563821  0.92287313\n",
      " 0.80319075 0.91280492        nan 0.76994665 0.92952232 0.92306306\n",
      " 0.92192335        nan        nan        nan        nan 0.91261477\n",
      " 0.85543276 0.91223512        nan        nan 0.91964383        nan\n",
      " 0.89513731        nan        nan 0.90330662]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'warm_start': False,\n",
       " 'solver': 'lbfgs',\n",
       " 'penalty': 'l2',\n",
       " 'max_iter': 486,\n",
       " 'C': 0.982}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.arange(0, 1, 0.001)\n",
    "max_iter = range(100, 500)\n",
    "warm_start = [True, False]\n",
    "solver = ['lbfgs', 'newton-cg', 'liblinear']\n",
    "penalty = ['l2', 'l1']\n",
    "\n",
    "params = {\n",
    "    'C' : C,\n",
    "    'max_iter' : max_iter,\n",
    "    'warm_start' : warm_start,\n",
    "    'solver' : solver,\n",
    "    'penalty' : penalty\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator = LogisticRegression(random_state = 1),\n",
    "    param_distributions = params,\n",
    "    n_iter = 100,\n",
    "    cv = 3,\n",
    "    n_jobs = -1,\n",
    "    random_state = 1,\n",
    "    verbose = 1\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775835866261399"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr = random_search.best_estimator_\n",
    "model_lr.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.95\n",
      "Cohen Kappa score: 0.92\n"
     ]
    }
   ],
   "source": [
    "predicted = model_lr.predict(x_val)\n",
    "\n",
    "lr_acc = accuracy_score(y_val,predicted)\n",
    "lr_cop = cohen_kappa_score(y_val,predicted)\n",
    "lr = pd.DataFrame([lr_acc, lr_cop], columns = ['Logistic Regression with RandomizedSearchCV'])\n",
    "\n",
    "print(\"Test score: {:.2f}\".format(lr_acc))\n",
    "print(\"Cohen Kappa score: {:.2f}\".format(lr_cop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_prior': True, 'alpha': 0.024}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = np.arange(0, 1, 0.001)\n",
    "fit_prior = [True, False]\n",
    "\n",
    "params = {\n",
    "    'alpha' : alpha,\n",
    "    'fit_prior' : fit_prior\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator = MultinomialNB(),\n",
    "    param_distributions = params,\n",
    "    n_iter = 100,\n",
    "    cv = 3,\n",
    "    n_jobs = -1,\n",
    "    random_state = 1,\n",
    "    verbose = 1\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9946808510638298"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mnb = random_search.best_estimator_\n",
    "model_mnb.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, ..., 2, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mnb.predict(x_val)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ac7df784fb9f66341270b4235b901a57e4c94a02ff00519a90c3611eb6bf397"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
